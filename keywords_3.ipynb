{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = '/Users/dariabakshandaeva/Documents/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [os.path.join(PATH_TO_DATA, file) for file in os.listdir(PATH_TO_DATA) if file.endswith('jsonlines')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([pd.read_json(file, lines=True) for file in files][:1], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(true_kws, predicted_kws):\n",
    "    assert len(true_kws) == len(predicted_kws)\n",
    "    \n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    jaccards = []\n",
    "    \n",
    "    for i in range(len(true_kws)):\n",
    "        true_kw = set(true_kws[i])\n",
    "        predicted_kw = set(predicted_kws[i])\n",
    "        \n",
    "        tp = len(true_kw & predicted_kw)\n",
    "        union = len(true_kw | predicted_kw)\n",
    "        fp = len(predicted_kw - true_kw)\n",
    "        fn = len(true_kw - predicted_kw)\n",
    "        \n",
    "        if (tp+fp) == 0:\n",
    "            prec = 0\n",
    "        else:\n",
    "            prec = tp / (tp + fp)\n",
    "        \n",
    "        if (tp+fn) == 0:\n",
    "            rec = 0\n",
    "        else:\n",
    "            rec = tp / (tp + fn)\n",
    "        if (prec+rec) == 0:\n",
    "            f1 = 0\n",
    "        else:\n",
    "            f1 = (2*(prec*rec))/(prec+rec)\n",
    "            \n",
    "        jac = tp / union\n",
    "        \n",
    "        precisions.append(prec)\n",
    "        recalls.append(rec)\n",
    "        f1s.append(f1)\n",
    "        jaccards.append(jac)\n",
    "    print('Precision - ', round(np.mean(precisions), 2))\n",
    "    print('Recall - ', round(np.mean(recalls), 2))\n",
    "    print('F1 - ', round(np.mean(f1s), 2))\n",
    "    print('Jaccard - ', round(np.mean(jaccards), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "теперь подготовим текст к использованию TermExtractor (избавимся от пунктуации и превратим в строки; стоп-слова убирать не нужно)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "punct = punctuation+'«»—…“”*№–'\n",
    "\n",
    "def words(text):\n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "    return words\n",
    "\n",
    "data['content_norm'] = data['content'].apply(words)\n",
    "\n",
    "data['content_norm_str'] = data['content_norm'].apply(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from rutermextract import TermExtractor\n",
    "term_extractor = TermExtractor()\n",
    "\n",
    "\n",
    "def extr(text):\n",
    "    words = [term.normalized for term in term_extractor(text)]\n",
    "    \n",
    "    return words\n",
    "\n",
    "\n",
    "data['keywords_extr'] = data['content_norm_str'].apply(extr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [ольга васильева, новые фгосы, историческое ис...\n",
       "1    [красота, ваша красота, руина, куда, глаза, са...\n",
       "2    [пепеляев, юзефович, якутия, места, книга, нач...\n",
       "Name: keywords_extr, dtype: object"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['keywords_extr'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.18\n",
      "Recall -  0.17\n",
      "F1 -  0.16\n",
      "Jaccard -  0.1\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], data['keywords_extr'].apply(lambda x: [x[0] for x in Counter(x).most_common(5)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "неплохо, но baseline не побило :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем NLTK Rake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['число книг',\n",
       " 'цели создания',\n",
       " 'участников событий',\n",
       " 'увязываю это',\n",
       " 'точки зрения',\n",
       " 'существующему стандарту',\n",
       " 'станет понятно',\n",
       " 'средней школы',\n",
       " 'сохранена структура',\n",
       " 'сонме романов',\n",
       " 'совокупность требований',\n",
       " 'собственное решение',\n",
       " 'собрание должно',\n",
       " 'своей страничке',\n",
       " 'самое важное',\n",
       " 'редакции фгоса',\n",
       " 'развивают память',\n",
       " 'против которого',\n",
       " 'пришло сообщение',\n",
       " 'прежней редакции',\n",
       " 'полностью отсутствовало',\n",
       " 'показала разницу',\n",
       " 'позицией министерства',\n",
       " 'подробных пункта',\n",
       " 'поверхностного разговора',\n",
       " 'оценивать содержащуюся',\n",
       " 'основного общего',\n",
       " 'основе план',\n",
       " 'однако ученики',\n",
       " 'начальной школы',\n",
       " 'науки рф',\n",
       " 'министерстве образования',\n",
       " 'место создания',\n",
       " 'любому родителю',\n",
       " 'которую развязали',\n",
       " 'историческом источнике',\n",
       " 'интересно знать',\n",
       " 'изучении событий',\n",
       " 'дополнительной информации',\n",
       " 'го класса',\n",
       " 'вчерашнем заседании',\n",
       " 'включая литературу',\n",
       " '– заявила',\n",
       " '– вспомните',\n",
       " 'школе ».',\n",
       " 'списку ».',\n",
       " 'основной школы',\n",
       " 'основной мысли',\n",
       " 'способность определять',\n",
       " 'принят фгос',\n",
       " '« кроме',\n",
       " '–',\n",
       " 'основной',\n",
       " '».',\n",
       " 'фгос',\n",
       " 'определять',\n",
       " '«',\n",
       " 'явлениях',\n",
       " 'явления',\n",
       " 'явлений',\n",
       " 'читают',\n",
       " 'читали',\n",
       " 'читала',\n",
       " 'числе',\n",
       " 'фгосам',\n",
       " 'уроке',\n",
       " 'темы',\n",
       " 'таблицу',\n",
       " 'схему',\n",
       " 'стихотворений',\n",
       " 'среднего',\n",
       " 'сохраняем',\n",
       " 'состояться',\n",
       " 'составлять',\n",
       " 'сопоставлять',\n",
       " 'совпадает',\n",
       " 'событиях',\n",
       " 'события',\n",
       " 'связи',\n",
       " 'ребенок',\n",
       " 'разные',\n",
       " 'прошлых',\n",
       " 'процитирую',\n",
       " 'процессы',\n",
       " 'проучившихся',\n",
       " 'проблема',\n",
       " 'понедельник',\n",
       " 'перенесено',\n",
       " 'отвечать',\n",
       " 'освоения',\n",
       " 'нацелены',\n",
       " 'настоящих',\n",
       " 'напомним',\n",
       " 'наверное',\n",
       " 'которых',\n",
       " 'которой',\n",
       " 'конкретные',\n",
       " 'конкретизирован',\n",
       " 'классы',\n",
       " 'классам',\n",
       " 'интеллект',\n",
       " 'изучаются',\n",
       " 'изучает',\n",
       " 'изменения',\n",
       " 'душу',\n",
       " 'думаю',\n",
       " 'др',\n",
       " 'далее',\n",
       " 'говорится',\n",
       " 'говорим',\n",
       " 'время',\n",
       " 'вопросы',\n",
       " 'войне',\n",
       " 'возражать',\n",
       " 'вкус',\n",
       " 'видится',\n",
       " 'вернулась',\n",
       " 'будем',\n",
       " 'беседе',\n",
       " 'анализировать',\n",
       " 'авторство',\n",
       " '6',\n",
       " '235']"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_list = pd.Series.tolist(data['content'])\n",
    "\n",
    "from rake_nltk import Metric, Rake\n",
    "\n",
    "r = Rake(language='russian', stopwords=stopwords.words('russian'), min_length=1, max_length=2) #берем униграммы и биграммы\n",
    "\n",
    "r.extract_keywords_from_text(content_list[0])\n",
    "r.get_ranked_phrases()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Униграммы и биграммы в большинстве своем нерелевантны, плохого качества"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
